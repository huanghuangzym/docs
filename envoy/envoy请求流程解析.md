  

*   1[流量劫持](#envoy请求流程解析-流量劫持)
    *   1.1[边车模式](#envoy请求流程解析-边车模式)
        *   1.1.1[拦截模式](#envoy请求流程解析-拦截模式)
        *   1.1.2[默认模式简介](#envoy请求流程解析-默认模式简介)
    *   1.2[网关模式](#envoy请求流程解析-网关模式)
*   2[请求解析](#envoy请求流程解析-请求解析)
    *   2.1[outbound方向](#envoy请求流程解析-outbound方向)
        *   2.1.1[启动监听](#envoy请求流程解析-启动监听)
            *   2.1.1.1[关于reuseport](#envoy请求流程解析-关于reuseport)
        *   2.1.2[建立连接](#envoy请求流程解析-建立连接)
        *   2.1.3[接收请求](#envoy请求流程解析-接收请求)
        *   2.1.4[发送请求](#envoy请求流程解析-发送请求)
        *   2.1.5[ 接收响应](#envoy请求流程解析-接收响应)
        *   2.1.6[返回响应](#envoy请求流程解析-返回响应)

  

流量劫持
====

在istio当中

envoy运行有两种模式，分别为边车模式和代理模式

边车模式
----

![](/root/Documents/tect/hl/image2021-12-6_14-39-57.png)

其中边车模式为通过iptable进行流量劫持

### 拦截模式

Istio支持两种拦截模式：

1.  REDIRECT：使用iptables的REDIRECT目标来拦截入站请求，转给Envoy，从Linux2.6.15的内核版本后，iptables开始支持状态跟踪(conntrack)，该功能依赖于netfilter的内核模块nf\_conntrack。此后，iptables可以根据包的状态进行二次的过滤拦截和状态跟踪。它也是state/ctstate和nat的主要依赖模块。
2.  TPROXY：使用iptables的TPROXY目标来拦截入站请求，tproxy 可以用于 inbound 流量的重定向，且无需改变报文中的目的 IP/端口，不需要执行连接跟踪，不会出现 conntrack 模块创建大量连接的问题。受限于内核版本，tproxy 应用于 outbound 存在一定缺陷。目前 Istio 支持通过 tproxy 处理 inbound 流量。

可以全局的设置默认拦截模式，也可以通过注解[sidecar.istio.io/interceptionMode](http://sidecar.istio.io/interceptionMode): TPROXY给某个工作负载单独设置。

无论采用哪种透明劫持方案，均需要解决获取真实目的 IP/端口的问题，使用 iptables 方案通过 getsockopt 方式获取，tproxy 可以直接读取目的地址

关于trpoxy

![](/root/Documents/tect/hl/image2021-12-10_14-56-37.png?version=1&modificationDate=1639119398000&api=v2)

[https://github.com/istio/istio/issues/5679](https://github.com/istio/istio/issues/5679)

  

关于netfilter和conntrack

[https://serverfault.com/questions/1030236/when-does-iptables-conntrack-module-track-states-of-packets](https://serverfault.com/questions/1030236/when-does-iptables-conntrack-module-track-states-of-packets)

关于为什么istio考虑inbound支持tproxy模式

如果我们是服务端，那么SYN包到达的时候，在POSTROUTING链的NAT表执行过之后（可能做DNAT或者REDIRECT），路由表将决定是FORWARD还是INPUT：

*   如果INPUT，那么conntrack记录就此生成，当回包的时候会首先根据conntrack作地址复原，并且是不会经过OUTPUT/POSTROUTING链NAT表的。
*   如果FORWARD，那么conntrack记录不会立即生成，需要经过POSTROUTING之后才知道是否做了SNAT/MASQUERADE，此时才会生成conntrack记录。当收到上游回包的时候，不会过PREROUTING的NAT表，而是直接根据conntrack复原为原始IP地址，然后直接FORWARD->POSTROUTING（不会过NAT表）送回原始客户端。

  

目前 Istio 使用 iptables 实现透明劫持，由于需要借助于 conntrack 模块实现连接跟踪，在连接数较多的情况下，会造成较大的消耗，同时可能会造成 track 表满的情况，为了避免这个问题，业内有ebpf的加速方案，报文可以不过内核协议栈进行加速穿越

关于track表的限制的资料

[https://www.cyberciti.biz/faq/ip\_conntrack-table-ful-dropping-packet-error/](https://www.cyberciti.biz/faq/ip_conntrack-table-ful-dropping-packet-error/)

  

### 默认模式简介

进入sidecar的网络空间

这里介绍的是iptables redirect模式

![](/root/Documents/tect/hl/image2021-12-6_11-31-29.png?version=1&modificationDate=1638763090000&api=v2)

![](/root/Documents/tect/hl/image2021-12-6_11-32-16.png?version=1&modificationDate=1638763090000&api=v2)

  

可见出口都redirect到15001当中，入口流量都被劫持到15006端口

```
# Generated by iptables-save v1.6.1 on Mon Dec  6 11:33:15 2021
*nat
:PREROUTING ACCEPT [3402:234907]
:INPUT ACCEPT [3167:190128]
:OUTPUT ACCEPT [529:49526]
:POSTROUTING ACCEPT [529:49526]
:ISTIO_INBOUND - [0:0]
:ISTIO_IN_REDIRECT - [0:0]
:ISTIO_OUTPUT - [0:0]
:ISTIO_REDIRECT - [0:0]
-A PREROUTING -p tcp -j ISTIO_INBOUND
-A OUTPUT -p tcp -j ISTIO_OUTPUT
-A OUTPUT -p udp -m udp --dport 53 -m owner --uid-owner 1337 -j RETURN
-A OUTPUT -p udp -m udp --dport 53 -m owner --gid-owner 1337 -j RETURN
-A OUTPUT -d 10.96.0.10/32 -p udp -m udp --dport 53 -j REDIRECT --to-ports 15053
-A ISTIO_INBOUND -p tcp -m tcp --dport 15008 -j RETURN
-A ISTIO_INBOUND -p tcp -m tcp --dport 22 -j RETURN
-A ISTIO_INBOUND -p tcp -m tcp --dport 15090 -j RETURN
-A ISTIO_INBOUND -p tcp -m tcp --dport 15021 -j RETURN
-A ISTIO_INBOUND -p tcp -m tcp --dport 15020 -j RETURN
-A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT
-A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15006
-A ISTIO_OUTPUT -s 127.0.0.6/32 -o lo -j RETURN
-A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -p tcp -m tcp ! --dport 53 -m owner --uid-owner 1337 -j ISTIO_IN_REDIRECT
-A ISTIO_OUTPUT -o lo -p tcp -m tcp ! --dport 53 -m owner ! --uid-owner 1337 -j RETURN
-A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN
-A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -m owner --gid-owner 1337 -j ISTIO_IN_REDIRECT
-A ISTIO_OUTPUT -o lo -p tcp -m tcp ! --dport 53 -m owner ! --gid-owner 1337 -j RETURN
-A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN
-A ISTIO_OUTPUT -d 10.96.0.10/32 -p tcp -m tcp --dport 53 -j REDIRECT --to-ports 15053
-A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN
-A ISTIO_OUTPUT -j ISTIO_REDIRECT
-A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001
COMMIT
# Completed on Mon Dec  6 11:33:15 2021
```

查看iptables规则（对iptables熟悉的小伙伴可以看到，除了截图的出口，入口流量的劫持，针对某些端口）

| ip端口  | 方向  | 动作  |
| --- | --- | --- |
| `10.96.0.10`/53 | 出站  | tcp/udp 都劫持到15053 |
| 15090 | 入站  | 不劫持 |
| 22  | 入站  | 不劫持 |
| 15020 | 入站  | 不劫持 |
| 15021 | 入站  | 不劫持 |

  

`10.96.0.10为k8s环境中dns服务器地址（默认为coredns的svc ip）由istio获得填充`

关于为什么iptable除了udp的53端口做拦截，对tcp的53也做了拦截

[https://github.com/istio/istio/pull/27081/file](https://github.com/istio/istio/pull/27081/file)  为了支持 dns over tcp

  

  

  

15001和15006分开

[https://github.com/istio/istio/pull/15713](https://github.com/istio/istio/pull/15713)  
  

  

除了其他一些针对特定uid用户的流量和lo口的return防止流量循环，

iptable规则中还出现了个127.0.0.6的地址，这里做出简单解释，参见：

[https://github.com/istio/istio/issues/29603](https://github.com/istio/istio/issues/29603)

![](/root/Documents/tect/hl/image2021-12-16_18-25-26.png?version=1&modificationDate=1639650327000&api=v2)

  

![](/root/Documents/tect/hl/image2021-12-9_14-41-52.png?version=1&modificationDate=1639032112000&api=v2)

![](/root/Documents/tect/hl/image2021-12-9_14-42-22.png?version=1&modificationDate=1639032143000&api=v2)

关于inbound的设计文档

[https://docs.google.com/document/d/1j-5\_XpeMTnT9mV\_8dbSOeU7rfH-5YNtN\_JJFZ2mmQ\_w/edit#heading=h.xw1gqgyqs5b](https://docs.google.com/document/d/1j-5_XpeMTnT9mV_8dbSOeU7rfH-5YNtN_JJFZ2mmQ_w/edit#heading=h.xw1gqgyqs5b)

  

  

  

[关于iptables](/pages/viewpage.action?pageId=98889098)

  

![](/root/Documents/tect/hl/image2021-12-7_10-21-3.png?version=1&modificationDate=1638843664000&api=v2)

  

参考：

[https://www.debuntu.org/how-to-redirecting-network-traffic-to-a-new-ip-using-iptables/](https://www.debuntu.org/how-to-redirecting-network-traffic-to-a-new-ip-using-iptables/)

  

网关模式
----

![](/root/Documents/tect/hl/image2021-12-6_14-40-37.png?version=1&modificationDate=1638772838000&api=v2)

网关模式并无iptables作流量劫持，listener也并非虚拟listener，敬请期待下一篇分析

 

请求解析
====
envoy当中基于libevent进行封装了各种文件，定时器事件等操作，以及dispatch对象的分发，和延迟析构，worker启动，worker listener绑定等部分不在这里作解读，后续有空可以单独再进行分析
跳过envoy当中的事件循环模型，这里以请求触发开始

outbound方向
----------

filter解析

![](/root/Documents/tect/hl/image2021-12-6_12-47-7.png?version=1&modificationDate=1638766027000&api=v2)

  

### 启动监听

1.  通过xDS或者静态配置，获得Envoy代理的监听器信息
2.  如果监听器bind\_to\_port，则直接调用libevent的接口，绑定监听，回调函数设置为ListenerImpl::listenCallback
    
    ```
    void ListenerManagerImpl::addListenerToWorker(Worker& worker,
                                                  absl::optional<uint64_t> overridden_listener,
                                                  ListenerImpl& listener,
                                                  ListenerCompletionCallback completion_callback) {
      if (overridden_listener.has_value()) {
        ENVOY_LOG(debug, "replacing existing listener {}", overridden_listener.value());
        worker.addListener(overridden_listener, listener, [this, completion_callback](bool) -> void {
          server_.dispatcher().post([this, completion_callback]() -> void {
            stats_.listener_create_success_.inc();
            if (completion_callback) {
              completion_callback();
            }
          });
        });
        return;
      }
      worker.addListener(
          overridden_listener, listener, [this, &listener, completion_callback](bool success) -> void {
            // The add listener completion runs on the worker thread. Post back to the main thread to
            // avoid locking.
            server_.dispatcher().post([this, success, &listener, completion_callback]() -> void {
     
     
     
     
    void ListenSocketImpl::setupSocket(const Network::Socket::OptionsSharedPtr& options,
                                       bool bind_to_port) {
      setListenSocketOptions(options);
     
      if (bind_to_port) {
        bind(address_provider_->localAddress());
      }
    }
     
     
    ActiveTcpListener::ActiveTcpListener(Network::TcpConnectionHandler& parent,
                                         Network::ListenerConfig& config)
        : ActiveTcpListener(
              parent,
              parent.dispatcher().createListener(config.listenSocketFactory().getListenSocket(), *this,
                                                 config.bindToPort(), config.tcpBacklogSize()),
              config) {}
     
     
    class ActiveTcpListener : public Network::TcpListenerCallbacks,
                              public ActiveListenerImplBase,
                              public Network::BalancedConnectionHandler,
                              Logger::Loggable<Logger::Id::conn_handler> {
    public:
      ActiveTcpListener(Network::TcpConnectionHandler& parent, Network::ListenerConfig& config);
      ActiveTcpListener(Network::TcpConnectionHandler& parent, Network::ListenerPtr&& listener,
                        Network::ListenerConfig& config);
      ~ActiveTcpListener() override;
      bool listenerConnectionLimitReached() const {
        // TODO(tonya11en): Delegate enforcement of per-listener connection limits to overload
        // manager.
        return !config_->openConnections().canCreate();
      }
     
      void decNumConnections() {
        ASSERT(num_listener_connections_ > 0);
        --num_listener_connections_;
        config_->openConnections().dec();
      }
     
      // Network::TcpListenerCallbacks
      void onAccept(Network::ConnectionSocketPtr&& socket) override;
      void onReject(RejectCause) override;
     
    listener_.reset(
          // libevent的base                      当前对象方法                   套接字的文件描述符
          evconnlistener_new(&dispatcher.base(), listenCallback, this, 0, -1, socket.ioHandle().fd()));
      
      if (!listener_) {
        throw CreateListenerException(
            fmt::format("cannot listen on socket: {}", socket.localAddress()->asString()));
      }
      
      if (!Network::Socket::applyOptions(socket.options(), socket,
                                         envoy::api::v3::core::SocketOption::STATE_LISTENING)) {
        throw CreateListenerException(fmt::format("cannot set post-listen socket option on socket: {}",
                                                  socket.localAddress()->asString()));
      }
      
      evconnlistener_set_error_cb(listener_.get(), errorCallback);
    
    ```

#### 关于reuseport

1.  [https://github.com/envoyproxy/envoy/issues/4602#issuecomment-544704931](https://github.com/envoyproxy/envoy/issues/4602#issuecomment-544704931)
2.  [https://github.com/envoyproxy/envoy/issues/8794](https://github.com/envoyproxy/envoy/issues/8794)
3.  [https://lwn.net/Articles/542629/](https://lwn.net/Articles/542629/)(实现与现有缺陷)
4.  [https://tech.flipkart.com/linux-tcp-so-reuseport-usage-and-implementation-6bfbf642885a](https://tech.flipkart.com/linux-tcp-so-reuseport-usage-and-implementation-6bfbf642885a)

![](/root/Documents/tect/hl/image2021-12-10_14-8-11.png?version=1&modificationDate=1639116492000&api=v2)

多个 server socket 监听相同的端口。每个 server socket 对应一个监听线程。内核 TCP 栈接收到客户端建立连接请求(SYN)时，按 TCP 4 元组(srcIP,srcPort,destIP,destPort) hash 算法，选择一个监听线程，唤醒之。新连接绑定到被唤醒的线程。**所以相对于非`SO_REUSEPORT`， 连接更为平均地分布到线程中（hash 算法不是绝对平均）**

  

envoy当中是支持在listener去设置开启这个特性，但是热重启场景时，对内核版本有一定要求（4.19-rc1）

[https://www.envoyproxy.io/docs/envoy/v1.18.3/api-v3/config/listener/v3/listener.proto](https://www.envoyproxy.io/docs/envoy/v1.18.3/api-v3/config/listener/v3/listener.proto)

![](/root/Documents/tect/hl/image2021-12-10_14-10-27.png?version=1&modificationDate=1639116628000&api=v2)

验证观察

默认未开启，通过envoyfilter进行开启后，可见15001的端口被开启

![](/root/Documents/tect/hl/image2021-12-10_14-14-2.png?version=1&modificationDate=1639116843000&api=v2)

  
```
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: reuseport
  namespace: testhl
spec:
  workloadSelector:
    labels:
      app: asm-0
  configPatches:
    - applyTo: LISTENER
      match:
        context: SIDECAR_OUTBOUND
        listener:
          portNumber: 15001
          name: "virtualOutbound"
      patch:
        operation: MERGE
        value:
          reuse_port: true
```

需要重启 POD

  

![](/root/Documents/tect/hl/image2021-12-10_14-19-14.png?version=1&modificationDate=1639117155000&api=v2)

  

而对于没有应用reuseport

![](/root/Documents/tect/hl/image2021-12-10_14-20-56.png?version=1&modificationDate=1639117256000&api=v2)

  

大致的平均

![](/root/Documents/tect/hl/image2021-12-10_15-54-23.png?version=1&modificationDate=1639122864000&api=v2)

  

_关于绝对的链接平衡， 可以试试 Listener 的配置`connection_balance_config： exact_balance`，不过由于有锁，对高频新连接应该有一定的性能损耗。目前只适用于 TCP 监听器_

![](/root/Documents/tect/hl/image2021-12-10_15-59-32.png?version=1&modificationDate=1639123172000&api=v2)

![](/root/Documents/tect/hl/image2021-12-10_15-59-48.png?version=1&modificationDate=1639123189000&api=v2)

  

代码部分：

```
Network::BalancedConnectionHandlerOptRef new_listener;
 
  if (hand_off_restored_destination_connections_ &&
      socket_->addressProvider().localAddressRestored()) {
    // Find a listener associated with the original destination address.
    new_listener =
        listener_.parent_.getBalancedHandlerByAddress(*socket_->addressProvider().localAddress());
  }
 
 
if (!rebalanced) {
    Network::BalancedConnectionHandler& target_handler =
        config_->connectionBalancer().pickTargetHandler(*this);
    if (&target_handler != this) {
      target_handler.post(std::move(socket));
      return;
    }
  }
 
  auto active_socket = std::make_unique<ActiveTcpSocket>(*this, std::move(socket),
                                                         hand_off_restored_destination_connections);
 
  // Create and run the filters
  config_->filterChainFactory().createListenerFilterChain(*active_socket);
  active_socket->continueFilterChain(true);
 
 
Network::BalancedConnectionHandlerOptRef
ConnectionHandlerImpl::getBalancedHandlerByAddress(const Network::Address::Instance& address) {
  // This is a linear operation, may need to add a map<address, listener> to improve performance.
  // However, linear performance might be adequate since the number of listeners is small.
  // We do not return stopped listeners.
  auto listener_it =
      std::find_if(listeners_.begin(), listeners_.end(),
                   [&address](std::pair<Network::Address::InstanceConstSharedPtr,
                                        ConnectionHandlerImpl::ActiveListenerDetails>& p) {
                     return p.second.tcpListener().has_value() &&
                            p.second.listener_->listener() != nullptr &&
                            p.first->type() == Network::Address::Type::Ip && *(p.first) == address;
                   });
 
  // If there is exact address match, return the corresponding listener.
  if (listener_it != listeners_.end()) {
    return Network::BalancedConnectionHandlerOptRef(
        listener_it->second.tcpListener().value().get());
  }
 
  // Otherwise, we need to look for the wild card match, i.e., 0.0.0.0:[address_port].
  // We do not return stopped listeners.
  // TODO(wattli): consolidate with previous search for more efficiency.
  if (Runtime::runtimeFeatureEnabled(
          "envoy.reloadable_features.listener_wildcard_match_ip_family")) {
    listener_it =
        std::find_if(listeners_.begin(), listeners_.end(),
                     [&address](const std::pair<Network::Address::InstanceConstSharedPtr,
                                                ConnectionHandlerImpl::ActiveListenerDetails>& p) {
```
  

### 建立连接

1.   DispatcherImpl通过libevent，接收到请求，调用ListenerImpl::listenCallback 
2.   client向envoy发起连接，envoy的worker接收eventloop的callback， 触发 Envoy::Network::ListenerImpl::listenCallback(port: 15001)  
3.  15001的`useOriginalDst": true`,`accept_filters_`中会带有`OriginalDstFilter`
4.  在`OriginalDstFilter.OnAccept`中用`os_syscalls.getsockopt(fd, SOL_IP, SO_ORIGINAL_DST, &orig_addr, &addr_len)`获取在iptables修改之前dst ip  [iptables与getsockopt](/pages/viewpage.action?pageId=98888685)
    ```
    Network::Address::InstanceConstSharedPtr OriginalDstFilter::getOriginalDst(Network::Socket& sock) {
      return Network::Utility::getOriginalDst(sock);
    }
     
       
     
    sockaddr_storage orig_addr;
      memset(&orig_addr, 0, sizeof(orig_addr));
      socklen_t addr_len = sizeof(sockaddr_storage);
      int status;
     
      if (*ipVersion == Address::IpVersion::v4) {
        status = sock.getSocketOption(SOL_IP, SO_ORIGINAL_DST, &orig_addr, &addr_len).rc_;
      } else {
        status = sock.getSocketOption(SOL_IPV6, IP6T_SO_ORIGINAL_DST, &orig_addr, &addr_len).rc_;
      }
     
      if (status != 0) {
        return nullptr;
      }
     
      return Address::addressFromSockAddr(orig_addr, 0, true /* default for v6 constructor */);
    ```
   
5.  在newconnection当中，还会通过 getBalancedHandlerByAddress寻找到实际的虚拟listener
6.  ```
    void ActiveTcpSocket::newConnection() {
      connected_ = true;
     
      // Check if the socket may need to be redirected to another listener.
      Network::BalancedConnectionHandlerOptRef new_listener;
     
      if (hand_off_restored_destination_connections_ &&
          socket_->addressProvider().localAddressRestored()) {
        // Find a listener associated with the original destination address.
        new_listener =
            listener_.parent_.getBalancedHandlerByAddress(*socket_->addressProvider().localAddress());
      }
    ```
    
7.  通过`` `ConnectionHandlerImpl::findActiveListenerByTag  
    ` ``
    ```
    Network::BalancedConnectionHandlerOptRef
    ConnectionHandlerImpl::getBalancedHandlerByAddress(const Network::Address::Instance& address) {
      // This is a linear operation, may need to add a map<address, listener> to improve performance.
      // However, linear performance might be adequate since the number of listeners is small.
      // We do not return stopped listeners.
      auto listener_it =
          std::find_if(listeners_.begin(), listeners_.end(),
                       [&address](std::pair<Network::Address::InstanceConstSharedPtr,
                                            ConnectionHandlerImpl::ActiveListenerDetails>& p) {
                         return p.second.tcpListener().has_value() &&
                                p.second.listener_->listener() != nullptr &&
                                p.first->type() == Network::Address::Type::Ip && *(p.first) == address;
                       });
     
      // If there is exact address match, return the corresponding listener.
      if (listener_it != listeners_.end()) {
        return Network::BalancedConnectionHandlerOptRef(
            listener_it->second.tcpListener().value().get());
      }
    ```
    
    查到addr对应的Listener
    
    1.  先查找`Listener.IP==addr.ip && Listener.Port==addr.port`的Listener
    2.  再查找`Listener.IP==0.0.0.0 && Listener.Port==addr.port`的Listener (对于tcp服务，ip会有值，对于http服务，ip为4个0)
8.  `dispatcher.createServerConnection`传入accept到的fd 创建Server连接对象`ConnectionImpl`， 并把`onFileEvent`注册到eventloop，等待读写事件的到来，因为socket是由一个non-blocking listening socket创建而来，所以也是non-blocking
9.  且注册的触发方式为epoll的边缘触发
    ```
    auto server_conn_ptr = parent_.dispatcher().createServerConnection(
          std::move(socket), std::move(transport_socket), *stream_info);
     
    Network::ServerConnectionPtr
    DispatcherImpl::createServerConnection(Network::ConnectionSocketPtr&& socket,
                                           Network::TransportSocketPtr&& transport_socket,
                                           StreamInfo::StreamInfo& stream_info) {
      ASSERT(isThreadSafe());
      return std::make_unique<Network::ServerConnectionImpl>(
          *this, std::move(socket), std::move(transport_socket), stream_info, true);
    }
     
    class ServerConnectionImpl : public ConnectionImpl, virtual public ServerConnection {
    public:
      ServerConnectionImpl(Event::Dispatcher& dispatcher, ConnectionSocketPtr&& socket,
                           TransportSocketPtr&& transport_socket, StreamInfo::StreamInfo& stream_info,
                           bool connected);
     
      // ServerConnection impl
      void setTransportSocketConnectTimeout(std::chrono::milliseconds timeout) override;
      void raiseEvent(ConnectionEvent event) override;
     
    private:
      void onTransportSocketConnectTimeout();
     
      bool transport_connect_pending_{true};
      // Implements a timeout for the transport socket signaling connection. The timer is enabled by a
      // call to setTransportSocketConnectTimeout and is reset when the connection is established.
      Event::TimerPtr transport_socket_connect_timer_;
    };
     
     
    Event::FileTriggerType trigger = Event::PlatformDefaultTriggerType;
     
      // We never ask for both early close and read at the same time. If we are reading, we want to
      // consume all available data.
      socket_->ioHandle().initializeFileEvent(
          dispatcher_, [this](uint32_t events) -> void { onFileEvent(events); }, trigger,
          Event::FileReadyType::Read | Event::FileReadyType::Write);
     
      transport_socket_->setTransportSocketCallbacks(*this);
     
     
    constexpr FileTriggerType determinePlatformPreferredEventType() {
    #if defined(WIN32) || defined(FORCE_LEVEL_EVENTS)
      return FileTriggerType::EmulatedEdge;
    #else
      return FileTriggerType::Edge;
    #endif
    }
     
    static constexpr FileTriggerType PlatformDefaultTriggerType = determinePlatformPreferredEventType();
    ```
    
10.  http的listener里filters为`envoy.http_connection_manager`，`buildFilterChain`里会把`HTTP::ConnectionManagerImpl`加入到`upstream_filters_`(list<ActiveReadFilterPtr>)中，这样在请求数据到达的时候，就可以使用http\_connection\_manager的`on_read`方法
11.  ```
     void FilterManagerImpl::addReadFilter(ReadFilterSharedPtr filter) {
       ASSERT(connection_.state() == Connection::State::Open);
       ActiveReadFilterPtr new_filter(new ActiveReadFilter{*this, filter});
       filter->initializeReadFilterCallbacks(*new_filter);
       LinkedList::moveIntoListBack(std::move(new_filter), upstream_filters_);
     }
      
      
     CodecClient::CodecClient(Type type, Network::ClientConnectionPtr&& connection,
                              Upstream::HostDescriptionConstSharedPtr host,
                              Event::Dispatcher& dispatcher)
         : type_(type), host_(host), connection_(std::move(connection)),
           idle_timeout_(host_->cluster().idleTimeout()) {
       if (type_ != Type::HTTP3) {
         // Make sure upstream connections process data and then the FIN, rather than processing
         // TCP disconnects immediately. (see https://github.com/envoyproxy/envoy/issues/1679 for
         // details)
         connection_->detectEarlyCloseWhenReadDisabled(false);
       }
       connection_->addConnectionCallbacks(*this);
       connection_->addReadFilter(Network::ReadFilterSharedPtr{new CodecReadFilter(*this)});
      
      
      connection_->noDelay(true);
     ```
    
12.  当连接刚刚加入eventloop的时候， Write Event会被立即触发，但因为`write_buffer_`没有数据，所以不会写入任何数据
13.  ```
     void CodecClient::onEvent(Network::ConnectionEvent event) {
       if (event == Network::ConnectionEvent::Connected) {
         ENVOY_CONN_LOG(debug, "connected", *connection_);
         connection_->streamInfo().setDownstreamSslConnection(connection_->ssl());
         connected_ = true;
       }
      
       if (event == Network::ConnectionEvent::RemoteClose) {
         remote_closed_ = true;
       }
      
       // HTTP/1 can signal end of response by disconnecting. We need to handle that case.
       if (type_ == Type::HTTP1 && event == Network::ConnectionEvent::RemoteClose &&
           !active_requests_.empty()) {
         Buffer::OwnedImpl empty;
         onData(empty);
       }
      
       if (event == Network::ConnectionEvent::RemoteClose ||
           event == Network::ConnectionEvent::LocalClose) {
         ENVOY_CONN_LOG(debug, "disconnect. resetting {} pending requests", *connection_,
                        active_requests_.size());
         disableIdleTimer();
         idle_timer_.reset();
         StreamResetReason reason = StreamResetReason::ConnectionFailure;
         if (connected_) {
           reason = StreamResetReason::ConnectionTermination;
           if (protocol_error_) {
             if (Runtime::runtimeFeatureEnabled(
                     "envoy.reloadable_features.return_502_for_upstream_protocol_errors")) {
               reason = StreamResetReason::ProtocolError;
               connection_->streamInfo().setResponseFlag(
                   StreamInfo::ResponseFlag::UpstreamProtocolError);
             }
           }
         }
         while (!active_requests_.empty()) {
           // Fake resetting all active streams so that reset() callbacks get invoked.
           active_requests_.front()->encoder_->getStream().resetStream(reason);
         }
       }
     }
     ```
    

### 接收请求

1.  client开始向socket写入请求数据
    
2.  eventloop在触发read event后，`transport_socket_.doRead`中会循环读取加入`read_buffer_`，直到返回EAGAIN
    
3. ```
   void ConnectionImpl::onReadReady() {
     ENVOY_CONN_LOG(trace, "read ready. dispatch_buffered_data={}", *this, dispatch_buffered_data_);
     const bool latched_dispatch_buffered_data = dispatch_buffered_data_;
     dispatch_buffered_data_ = false;
    
     ASSERT(!connecting_);
    
     // We get here while read disabled in two ways.
     // 1) There was a call to setTransportSocketIsReadable(), for example if a raw buffer socket ceded
     //    due to shouldDrainReadBuffer(). In this case we defer the event until the socket is read
     //    enabled.
     // 2) The consumer of connection data called readDisable(true), and instead of reading from the
     //    socket we simply need to dispatch already read data.
     if (read_disable_count_ != 0) {
       // Do not clear transport_wants_read_ when returning early; the early return skips the transport
       // socket doRead call.
       if (latched_dispatch_buffered_data && filterChainWantsData()) {
         onRead(read_buffer_->length());
       }
       return;
     }
    
     // Clear transport_wants_read_ just before the call to doRead. This is the only way to ensure that
     // the transport socket read resumption happens as requested; onReadReady() returns early without
     // reading from the transport if the read buffer is above high watermark at the start of the
     // method.
     transport_wants_read_ = false;
     IoResult result = transport_socket_->doRead(*read_buffer_);
     uint64_t new_buffer_size = read_buffer_->length();
     updateReadBufferStats(result.bytes_processed_, new_buffer_size);
    
     // If this connection doesn't have half-close semantics, translate end_stream into
     // a connection close.
     if ((!enable_half_close_ && result.end_stream_read_)) {
       result.end_stream_read_ = false;
       result.action_ = PostIoAction::Close;
     }
    
     read_end_stream_ |= result.end_stream_read_;
     if (result.bytes_processed_ != 0 || result.end_stream_read_ ||
         (latched_dispatch_buffered_data && read_buffer_->length() > 0)) {
       // Skip onRead if no bytes were processed unless we explicitly want to force onRead for
       // buffered data. For instance, skip onRead if the connection was closed without producing
       // more data.
       onRead(new_buffer_size);
     }
    
     // The read callback may have already closed the connection.
     if (result.action_ == PostIoAction::Close || bothSidesHalfClosed()) {
       ENVOY_CONN_LOG(debug, "remote close", *this);
       closeSocket(ConnectionEvent::RemoteClose);
     }
   }
   ``` 
    
4.  把buffer传入`Envoy::Http::ConnectionManagerImpl::onData`进行HTTP请求的处理
    
5.  ```
    Network::FilterStatus ConnectionManagerImpl::onData(Buffer::Instance& data, bool) {
      if (!codec_) {
        // Http3 codec should have been instantiated by now.
        createCodec(data);
      }
     
      bool redispatch;
      do {
        redispatch = false;
     
        const Status status = codec_->dispatch(data);
     
        if (isBufferFloodError(status) || isInboundFramesWithEmptyPayloadError(status)) {
          handleCodecError(status.message());
          return Network::FilterStatus::StopIteration;
        } else if (isCodecProtocolError(status)) {
          stats_.named_.downstream_cx_protocol_error_.inc();
          handleCodecError(status.message());
          return Network::FilterStatus::StopIteration;
        }
        ASSERT(status.ok());
    ```
    
6.  如果`codec_type`是AUTO（HTTP1,2，3目前还不支持，在计划中）的情况下，会判断请求是否以`PRI * HTTP/2为开始`来判断是否http2
    ```
    Http::ServerConnectionPtr
    HttpConnectionManagerConfig::createCodec(Network::Connection& connection,
                                             const Buffer::Instance& data,
                                             Http::ServerConnectionCallbacks& callbacks) {
      switch (codec_type_) {
      case CodecType::HTTP1: {
        return std::make_unique<Http::Http1::ServerConnectionImpl>(
            connection, Http::Http1::CodecStats::atomicGet(http1_codec_stats_, context_.scope()),
            callbacks, http1_settings_, maxRequestHeadersKb(), maxRequestHeadersCount(),
            headersWithUnderscoresAction());
      }
      case CodecType::HTTP2: {
        return std::make_unique<Http::Http2::ServerConnectionImpl>(
            connection, callbacks,
            Http::Http2::CodecStats::atomicGet(http2_codec_stats_, context_.scope()),
            context_.api().randomGenerator(), http2_options_, maxRequestHeadersKb(),
            maxRequestHeadersCount(), headersWithUnderscoresAction());
      }
      case CodecType::HTTP3:
    #ifdef ENVOY_ENABLE_QUIC
        return std::make_unique<Quic::QuicHttpServerConnectionImpl>(
            dynamic_cast<Quic::EnvoyQuicServerSession&>(connection), callbacks,
            Http::Http3::CodecStats::atomicGet(http3_codec_stats_, context_.scope()), http3_options_,
            maxRequestHeadersKb(), headersWithUnderscoresAction());
    #else
        // Should be blocked by configuration checking at an earlier point.
        NOT_REACHED_GCOVR_EXCL_LINE;
    #endif
      case CodecType::AUTO:
        return Http::ConnectionManagerUtility::autoCreateCodec(
            connection, data, callbacks, context_.scope(), context_.api().randomGenerator(),
            http1_codec_stats_, http2_codec_stats_, http1_settings_, http2_options_,
            maxRequestHeadersKb(), maxRequestHeadersCount(), headersWithUnderscoresAction());
      }
      NOT_REACHED_GCOVR_EXCL_LINE;
    }
     
     
    std::string ConnectionManagerUtility::determineNextProtocol(Network::Connection& connection,
                                                                const Buffer::Instance& data) {
      if (!connection.nextProtocol().empty()) {
        return connection.nextProtocol();
      }
     
      // See if the data we have so far shows the HTTP/2 prefix. We ignore the case where someone sends
      // us the first few bytes of the HTTP/2 prefix since in all public cases we use SSL/ALPN. For
      // internal cases this should practically never happen.
      if (data.startsWith(Http2::CLIENT_MAGIC_PREFIX)) {
        return Utility::AlpnNames::get().Http2;
      }
     
      return "";
    }
     
     
    const std::string CLIENT_MAGIC_PREFIX = "PRI * HTTP/2";
    ```
    
7.  利用`http_parser`进行http解析的callback,`ConnectionImpl::settings_`静态初始化了parse各个阶段的callbacks
    
8.  ```
    http_parser_settings ConnectionImpl::settings_{
        [](http_parser* parser) -> int {
          static_cast<ConnectionImpl*>(parser->data)->onMessageBeginBase();
          return 0;
        },
        [](http_parser* parser, const char* at, size_t length) -> int {
          static_cast<ConnectionImpl*>(parser->data)->onUrl(at, length);
          return 0;
        },
        nullptr, // on_status
        [](http_parser* parser, const char* at, size_t length) -> int {
          static_cast<ConnectionImpl*>(parser->data)->onHeaderField(at, length);
          return 0;
        },
        [](http_parser* parser, const char* at, size_t length) -> int {
          static_cast<ConnectionImpl*>(parser->data)->onHeaderValue(at, length);
          return 0;
        },
        [](http_parser* parser) -> int {
          return static_cast<ConnectionImpl*>(parser->data)->onHeadersCompleteBase();
        },
        [](http_parser* parser, const char* at, size_t length) -> int {
          static_cast<ConnectionImpl*>(parser->data)->onBody(at, length);
          return 0;
        },
        [](http_parser* parser) -> int {
          static_cast<ConnectionImpl*>(parser->data)->onMessageCompleteBase();
          return 0;
        },
        nullptr, // on_chunk_header
        nullptr  // on_chunk_complete
    };
    ```
   

1.  (envoy社区有讨论会将协议解析器从http\_parser换成llhttp)
2.  ![](/root/Documents/tect/hl/image2021-12-9_18-41-1.png?version=1&modificationDate=1639046462000&api=v2)
    

[https://github.com/envoyproxy/envoy/issues/5155](https://github.com/envoyproxy/envoy/issues/5155)

[https://github.com/envoyproxy/envoy/pull/15263/files](https://github.com/envoyproxy/envoy/pull/15263/files)  使用解析器接口，重构http parser

[https://github.com/envoyproxy/envoy/pull/15814](https://github.com/envoyproxy/envoy/pull/15814)       添加llhttp解析器的实现，暂时还没合并

  

1.  ```
    if (pos != absl::string_view::npos) {
          // Include \r or \n
          new_data = new_data.substr(0, pos + 1);
          ssize_t rc = http_parser_execute(&parser_, &settings_, new_data.data(), new_data.length());
          ENVOY_LOG(trace, "http inspector: http_parser parsed {} chars, error code: {}", rc,
                    HTTP_PARSER_ERRNO(&parser_));
     
          // Errors in parsing HTTP.
          if (HTTP_PARSER_ERRNO(&parser_) != HPE_OK && HTTP_PARSER_ERRNO(&parser_) != HPE_PAUSED) {
            return ParseState::Error;
          }
     
          if (parser_.http_major == 1 && parser_.http_minor == 1) {
            protocol_ = Http::Headers::get().ProtocolStrings.Http11String;
          } else {
            // Set other HTTP protocols to HTTP/1.0
            protocol_ = Http::Headers::get().ProtocolStrings.Http10String;
          }
          return ParseState::Done;
        } else {
          ssize_t rc = http_parser_execute(&parser_, &settings_, new_data.data(), new_data.length());
          ENVOY_LOG(trace, "http inspector: http_parser parsed {} chars, error code: {}", rc,
                    HTTP_PARSER_ERRNO(&parser_));
     
          // Errors in parsing HTTP.
          if (HTTP_PARSER_ERRNO(&parser_) != HPE_OK && HTTP_PARSER_ERRNO(&parser_) != HPE_PAUSED) {
            return ParseState::Error;
          } else {
            return ParseState::Continue;
          }
     
     
     
     
        return {http_parser_execute(&parser_, &settings_, slice, len), HTTP_PARSER_ERRNO(&parser_)};
    ```
    
2.  `onMessageBeginBase`
    
3.  ```
     current_header_map_ = std::make_unique<HeaderMapImpl>();
      header_parsing_state_ = HeaderParsingState::Field;
     
     
     
    Status ConnectionImpl::onMessageBegin() {
      ENVOY_CONN_LOG(trace, "message begin", connection_);
      // Make sure that if HTTP/1.0 and HTTP/1.1 requests share a connection Envoy correctly sets
      // protocol for each request. Envoy defaults to 1.1 but sets the protocol to 1.0 where applicable
      // in onHeadersCompleteBase
      protocol_ = Protocol::Http11;
      processing_trailers_ = false;
      header_parsing_state_ = HeaderParsingState::Field;
      allocHeaders(statefulFormatterFromSettings(codec_settings_));
      return onMessageBeginBase();
    }
     
    Status ServerConnectionImpl::onMessageBeginBase() {
      if (!resetStreamCalled()) {
        ASSERT(!active_request_.has_value());
        active_request_.emplace(*this);
        auto& active_request = active_request_.value();
        if (resetStreamCalled()) {
          return codecClientError("cannot create new streams after calling reset");
        }
        active_request.request_decoder_ = &callbacks_.newStream(active_request.response_encoder_);
     
        // Check for pipelined request flood as we prepare to accept a new request.
        // Parse errors that happen prior to onMessageBegin result in stream termination, it is not
        // possible to overflow output buffers with early parse errors.
        RETURN_IF_ERROR(doFloodProtectionChecks());
      }
      return okStatus();
    }
    ```
    
4.    
    
    1.  创建`ActiveStream`, 保存downstream的信息，和对应的route信息
    2.  对于https，会把TLS握手的时候保存的SNI写入`ActiveStream.requested_server_name_`
    3.  ```
        void setRequestedServerName(absl::string_view requested_server_name) override {
            requested_server_name_ = std::string(requested_server_name);
          }
         
        void Filter::onServername(absl::string_view name) {
          if (!name.empty()) {
            config_->stats().sni_found_.inc();
            cb_->socket().setRequestedServerName(name);
            ENVOY_LOG(debug, "tls:onServerName(), requestedServerName: {}", name);
          } else {
            config_->stats().sni_not_found_.inc();
          }
          clienthello_success_ = true;
        }
        ```
        
5.  `onHeaderField`,`onHeaderValue`
    
    迭代添加header到`current_header_map_`中
    
6.  `解析完最后一个请求头后会执行 onHeadersComplete`
    
    把request中的一些字段（method, path, host ）加入headers中
    
   ```
   const Http::HeaderValues& header_values = Http::Headers::get();
   active_request.response_encoder_.setIsResponseToHeadRequest(parser_->methodName() ==
                                                               header_values.MethodValues.Head);
   active_request.response_encoder_.setIsResponseToConnectRequest(
       parser_->methodName() == header_values.MethodValues.Connect);
    
   RETURN_IF_ERROR(handlePath(*headers, parser_->methodName()));
   ASSERT(active_request.request_url_.empty());
    
   headers->setMethod(parser_->methodName());
   headers->setScheme("http"); 
   ```
    
7.  ` 回调onHeadersComplete， 依次回调onMessageComplete，onMessageCompleteBase，ServerConnectionImpl::onMessageComplete`
    
    1.    
        
    2.  这个请求解码是Envoy上下文的，它会执行Envoy的核心代理逻辑 —— 遍历HTTP过滤器链、进行路由选择
        
    3.  此过滤器当中判断请求过载
    4.  通过route上的cluster name从ThreadLocalClusterManager中查找cluster, 缓存在`cached_cluster_info_`中
        
    5.  根据配置构造在route上的filterChain (具体的filter实现是通过`registerFactory`方法注册进去，在`createFilterChain`的时候根据名称构造，比如istio-proxy的stats)
        
    6.  如果对应http connection manager上有trace配置
        
    7.  ```
        if (connection_manager_.config_.tracingConfig()) {
          traceRequest();
        }
        ```
        
        1.  request header中有trace，就创建子span, sampled跟随parent span
        2.  如果header中没有trace，就创建root span, 并设置sampled
    8.  ```
        void ConnectionManagerImpl::ActiveStream::refreshCachedRoute(const Router::RouteCallback& cb) {
          Router::RouteConstSharedPtr route;
          if (request_headers_ != nullptr) {
            if (connection_manager_.config_.isRoutable() &&
                connection_manager_.config_.scopedRouteConfigProvider() != nullptr) {
              // NOTE: re-select scope as well in case the scope key header has been changed by a filter.
              snapScopedRouteConfig();
            }
            if (snapped_route_config_ != nullptr) {
              route = snapped_route_config_->route(cb, *request_headers_, filter_manager_.streamInfo(),
                                                   stream_id_);
            }
          }
         
          setRoute(route);
        }
         
        void ConnectionManagerImpl::ActiveStream::decodeHeaders(RequestHeaderMapPtr&& headers,
                                                                bool end_stream) {
        ScopeTrackerScopeState scope(this,
                                       connection_manager_.read_callbacks_->connection().dispatcher());
          request_headers_ = std::move(headers);
          filter_manager_.requestHeadersInitialized();
          if (request_header_timer_ != nullptr) {
            request_header_timer_->disableTimer();
            request_header_timer_.reset();
          }
         
          Upstream::HostDescriptionConstSharedPtr upstream_host =
              connection_manager_.read_callbacks_->upstreamHost();
         
          if (upstream_host != nullptr) {
            Upstream::ClusterRequestResponseSizeStatsOptRef req_resp_stats =
                upstream_host->cluster().requestResponseSizeStats();
            if (req_resp_stats.has_value()) {
              req_resp_stats->get().upstream_rq_headers_size_.recordValue(request_headers_->byteSize());
            }
          }
         
          // Both saw_connection_close_ and is_head_request_ affect local replies: set
          // them as early as possible.
          const Protocol protocol = connection_manager_.codec_->protocol();
          state_.saw_connection_close_ = HeaderUtility::shouldCloseConnection(protocol, *request_headers_);
         
          // We need to snap snapped_route_config_ here as it's used in mutateRequestHeaders later.
          if (connection_manager_.config_.isRoutable()) {
            if (connection_manager_.config_.routeConfigProvider() != nullptr) {
              snapped_route_config_ = connection_manager_.config_.routeConfigProvider()->config();
            } else if (connection_manager_.config_.scopedRouteConfigProvider() != nullptr) {
              snapped_scoped_routes_config_ =
                  connection_manager_.config_.scopedRouteConfigProvider()->config<Router::ScopedConfig>();
              snapScopedRouteConfig();
            }
          } else {
            snapped_route_config_ = connection_manager_.config_.routeConfigProvider()->config();
          }
         
          ENVOY_STREAM_LOG(debug, "request headers complete (end_stream={}):\n{}", *this, end_stream,
                           *request_headers_);
         
          // We end the decode here only if the request is header only. If we convert the request to a
          // header only, the stream will be marked as done once a subsequent decodeData/decodeTrailers is
          // called with end_stream=true.
          filter_manager_.maybeEndDecode(end_stream);
         
          // Drop new requests when overloaded as soon as we have decoded the headers.
          if (connection_manager_.random_generator_.bernoulli(
                  connection_manager_.overload_stop_accepting_requests_ref_.value())) {
            // In this one special case, do not create the filter chain. If there is a risk of memory
            // overload it is more important to avoid unnecessary allocation than to create the filters.
            filter_manager_.skipFilterChainCreation();
            connection_manager_.stats_.named_.downstream_rq_overload_close_.inc();
            sendLocalReply(Grpc::Common::isGrpcRequestHeaders(*request_headers_),
                           Http::Code::ServiceUnavailable, "envoy overloaded", nullptr, absl::nullopt,
                           StreamInfo::ResponseCodeDetails::get().Overload);
            return;
          }
         
          if (!connection_manager_.config_.proxy100Continue() && request_headers_->Expect() &&
              request_headers_->Expect()->value() == Headers::get().ExpectValues._100Continue.c_str()) {
            // Note in the case Envoy is handling 100-Continue complexity, it skips the filter chain
            // and sends the 100-Continue directly to the encoder.
            chargeStats(continueHeader());
            response_encoder_->encode100ContinueHeaders(continueHeader());
            // Remove the Expect header so it won't be handled again upstream.
            request_headers_->removeExpect();
          }
         
          connection_manager_.user_agent_.initializeFromHeaders(*request_headers_,
                                                                connection_manager_.stats_.prefixStatName(),
                                                                connection_manager_.stats_.scope_);
         
          // Make sure we are getting a codec version we support.
          if (protocol == Protocol::Http10) {
            // Assume this is HTTP/1.0. This is fine for HTTP/0.9 but this code will also affect any
            // requests with non-standard version numbers (0.9, 1.3), basically anything which is not
            // HTTP/1.1.
            //
            // The protocol may have shifted in the HTTP/1.0 case so reset it.
            filter_manager_.streamInfo().protocol(protocol);
            if (!connection_manager_.config_.http1Settings().accept_http_10_) {
              // Send "Upgrade Required" if HTTP/1.0 support is not explicitly configured on.
              sendLocalReply(false, Code::UpgradeRequired, "", nullptr, absl::nullopt,
                             StreamInfo::ResponseCodeDetails::get().LowVersion);
              return;
            }
            if (!request_headers_->Host() &&
                !connection_manager_.config_.http1Settings().default_host_for_http_10_.empty()) {
              // Add a default host if configured to do so.
              request_headers_->setHost(
                  connection_manager_.config_.http1Settings().default_host_for_http_10_);
            }
          }
         
          if (!request_headers_->Host()) {
            // Require host header. For HTTP/1.1 Host has already been translated to :authority.
            sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::BadRequest, "",
                           nullptr, absl::nullopt, StreamInfo::ResponseCodeDetails::get().MissingHost);
            return;
          }
         
          // Verify header sanity checks which should have been performed by the codec.
          ASSERT(HeaderUtility::requestHeadersValid(*request_headers_).has_value() == false);
         
          // Check for the existence of the :path header for non-CONNECT requests, or present-but-empty
          // :path header for CONNECT requests. We expect the codec to have broken the path into pieces if
          // applicable. NOTE: Currently the HTTP/1.1 codec only does this when the allow_absolute_url flag
          // is enabled on the HCM.
          if ((!HeaderUtility::isConnect(*request_headers_) || request_headers_->Path()) &&
              request_headers_->getPathValue().empty()) {
            sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::NotFound, "", nullptr,
                           absl::nullopt, StreamInfo::ResponseCodeDetails::get().MissingPath);
            return;
          }
         
          // Currently we only support relative paths at the application layer.
          if (!request_headers_->getPathValue().empty() && request_headers_->getPathValue()[0] != '/') {
            connection_manager_.stats_.named_.downstream_rq_non_relative_path_.inc();
            sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::NotFound, "", nullptr,
                           absl::nullopt, StreamInfo::ResponseCodeDetails::get().AbsolutePath);
            return;
          }
         
          // Path sanitization should happen before any path access other than the above sanity check.
          const auto action =
              ConnectionManagerUtility::maybeNormalizePath(*request_headers_, connection_manager_.config_);
          // gRPC requests are rejected if Envoy is configured to redirect post-normalization. This is
          // because gRPC clients do not support redirect.
          if (action == ConnectionManagerUtility::NormalizePathAction::Reject ||
              (action == ConnectionManagerUtility::NormalizePathAction::Redirect &&
               Grpc::Common::hasGrpcContentType(*request_headers_))) {
            connection_manager_.stats_.named_.downstream_rq_failed_path_normalization_.inc();
            sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::BadRequest, "",
                           nullptr, absl::nullopt,
                           StreamInfo::ResponseCodeDetails::get().PathNormalizationFailed);
            return;
          } else if (action == ConnectionManagerUtility::NormalizePathAction::Redirect) {
            connection_manager_.stats_.named_.downstream_rq_redirected_with_normalized_path_.inc();
            sendLocalReply(
                false, Code::TemporaryRedirect, "",
                [new_path = request_headers_->Path()->value().getStringView()](
                    Http::ResponseHeaderMap& response_headers) -> void {
                  response_headers.addReferenceKey(Http::Headers::get().Location, new_path);
                },
                absl::nullopt, StreamInfo::ResponseCodeDetails::get().PathNormalizationFailed);
            return;
          }
         
          ASSERT(action == ConnectionManagerUtility::NormalizePathAction::Continue);
          ConnectionManagerUtility::maybeNormalizeHost(*request_headers_, connection_manager_.config_,
                                                       localPort());
         
          if (!state_.is_internally_created_) { // Only sanitize headers on first pass.
            // Modify the downstream remote address depending on configuration and headers.
            filter_manager_.setDownstreamRemoteAddress(ConnectionManagerUtility::mutateRequestHeaders(
                *request_headers_, connection_manager_.read_callbacks_->connection(),
                connection_manager_.config_, *snapped_route_config_, connection_manager_.local_info_));
          }
          ASSERT(filter_manager_.streamInfo().downstreamAddressProvider().remoteAddress() != nullptr);
         
          ASSERT(!cached_route_);
          refreshCachedRoute();
         
          if (!state_.is_internally_created_) { // Only mutate tracing headers on first pass.
            filter_manager_.streamInfo().setTraceReason(
                ConnectionManagerUtility::mutateTracingRequestHeader(
                    *request_headers_, connection_manager_.runtime_, connection_manager_.config_,
                    cached_route_.value().get()));
          }
         
          filter_manager_.streamInfo().setRequestHeaders(*request_headers_);
         
          const bool upgrade_rejected = filter_manager_.createFilterChain() == false;
         
          // TODO if there are no filters when starting a filter iteration, the connection manager
          // should return 404. The current returns no response if there is no router filter.
          if (hasCachedRoute()) {
            // Do not allow upgrades if the route does not support it.
            if (upgrade_rejected) {
              // While downstream servers should not send upgrade payload without the upgrade being
              // accepted, err on the side of caution and refuse to process any further requests on this
              // connection, to avoid a class of HTTP/1.1 smuggling bugs where Upgrade or CONNECT payload
              // contains a smuggled HTTP request.
              state_.saw_connection_close_ = true;
              connection_manager_.stats_.named_.downstream_rq_ws_on_non_ws_route_.inc();
              sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::Forbidden, "",
                             nullptr, absl::nullopt, StreamInfo::ResponseCodeDetails::get().UpgradeFailed);
              return;
            }
            // Allow non websocket requests to go through websocket enabled routes.
          }
         
          if (hasCachedRoute()) {
            const Router::RouteEntry* route_entry = cached_route_.value()->routeEntry();
            if (route_entry != nullptr && route_entry->idleTimeout()) {
              // TODO(mattklein123): Technically if the cached route changes, we should also see if the
              // route idle timeout has changed and update the value.
              idle_timeout_ms_ = route_entry->idleTimeout().value();
              response_encoder_->getStream().setFlushTimeout(idle_timeout_ms_);
              if (idle_timeout_ms_.count()) {
                // If we have a route-level idle timeout but no global stream idle timeout, create a timer.
                if (stream_idle_timer_ == nullptr) {
                  stream_idle_timer_ =
                      connection_manager_.read_callbacks_->connection().dispatcher().createScaledTimer(
                          Event::ScaledTimerType::HttpDownstreamIdleStreamTimeout,
                          [this]() -> void { onIdleTimeout(); });
                }
              } else if (stream_idle_timer_ != nullptr) {
                // If we had a global stream idle timeout but the route-level idle timeout is set to zero
                // (to override), we disable the idle timer.
                stream_idle_timer_->disableTimer();
                stream_idle_timer_ = nullptr;
              }
            }
          }
         
          // Check if tracing is enabled at all.
          if (connection_manager_.config_.tracingConfig()) {
            traceRequest();
          }
         
          filter_manager_.decodeHeaders(*request_headers_, end_stream);
         
          // Reset it here for both global and overridden cases.
          resetIdleTimer();
        }
         
         
        void FilterManager::decodeHeaders(ActiveStreamDecoderFilter* filter, RequestHeaderMap& headers,
                                          bool end_stream) {
          // Headers filter iteration should always start with the next filter if available.
          std::list<ActiveStreamDecoderFilterPtr>::iterator entry =
              commonDecodePrefix(filter, FilterIterationStartState::AlwaysStartFromNext);
          std::list<ActiveStreamDecoderFilterPtr>::iterator continue_data_entry = decoder_filters_.end();
         
          for (; entry != decoder_filters_.end(); entry++) {
            (*entry)->maybeEvaluateMatchTreeWithNewData(
                [&](auto& matching_data) { matching_data.onRequestHeaders(headers); });
         
            if ((*entry)->skipFilter()) {
              continue;
            }
        ```
        
    9.  根据http connection manager上配置的filters (`envoy.cors`,`envoy.fault`,`envoy.router`)，一个个执行`decodeHeaders`
        
        这里主要写一下和`envoy.router`
        
        1.  `envoy.router`
            
            1.  在构造`RouteMatcher`的时候会遍历`virtual_hosts`下的domains，并根据通配符的位置和domain的长度分为4个`map<domain_len, std::unordered_map<domain, virtualHost>, std::greater<int64_t>>`
                
                *   `default_virtual_host_`domain就是一个通配符(只允许存在一个)
                    
                *   `wildcard_virtual_host_suffixes_`domain中通配符在开头
                    
                *   `wildcard_virtual_host_prefixes_`domain中通配符在结尾
                    
                *   `virtual_hosts_`不包含通配
                    
                *   ```
                    RouteMatcher::RouteMatcher(const envoy::config::route::v3::RouteConfiguration& route_config,
                                               const ConfigImpl& global_route_config,
                                               Server::Configuration::ServerFactoryContext& factory_context,
                                               ProtobufMessage::ValidationVisitor& validator, bool validate_clusters)
                        : vhost_scope_(factory_context.scope().scopeFromStatName(
                              factory_context.routerContext().virtualClusterStatNames().vhost_)) {
                      absl::optional<Upstream::ClusterManager::ClusterInfoMaps> validation_clusters;
                      if (validate_clusters) {
                        validation_clusters = factory_context.clusterManager().clusters();
                      }
                      for (const auto& virtual_host_config : route_config.virtual_hosts()) {
                        VirtualHostSharedPtr virtual_host(new VirtualHostImpl(virtual_host_config, global_route_config,
                                                                              factory_context, *vhost_scope_, validator,
                                                                              validation_clusters));
                        for (const std::string& domain_name : virtual_host_config.domains()) {
                          const std::string domain = Http::LowerCaseString(domain_name).get();
                          bool duplicate_found = false;
                          if ("*" == domain) {
                            if (default_virtual_host_) {
                              throw EnvoyException(fmt::format("Only a single wildcard domain is permitted in route {}",
                                                               route_config.name()));
                            }
                            default_virtual_host_ = virtual_host;
                          } else if (!domain.empty() && '*' == domain[0]) {
                            duplicate_found = !wildcard_virtual_host_suffixes_[domain.size() - 1]
                                                   .emplace(domain.substr(1), virtual_host)
                                                   .second;
                          } else if (!domain.empty() && '*' == domain[domain.size() - 1]) {
                            duplicate_found = !wildcard_virtual_host_prefixes_[domain.size() - 1]
                                                   .emplace(domain.substr(0, domain.size() - 1), virtual_host)
                                                   .second;
                          } else {
                            duplicate_found = !virtual_hosts_.emplace(domain, virtual_host).second;
                          }
                          if (duplicate_found) {
                            throw EnvoyException(fmt::format("Only unique values for domains are permitted. Duplicate "
                                                             "entry of domain {} in route {}",
                                                             domain, route_config.name()));
                          }
                        }
                      }
                    }
                    ```
                    
            2.  按照`virtual_hosts_`\=>`wildcard_virtual_host_suffixes_`\=>`wildcard_virtual_host_prefixes_`\=>`default_virtual_host_`的顺序查找
                
                同时按照map的迭代顺序（domain len降序）查找最先除去通配符后能匹配到的virtualhost，如果没有直接返回 404
                
            3.  ```
                const VirtualHostImpl* RouteMatcher::findVirtualHost(const Http::RequestHeaderMap& headers) const {
                  // Fast path the case where we only have a default virtual host.
                  if (virtual_hosts_.empty() && wildcard_virtual_host_suffixes_.empty() &&
                      wildcard_virtual_host_prefixes_.empty()) {
                    return default_virtual_host_.get();
                  }
                 
                  // There may be no authority in early reply paths in the HTTP connection manager.
                  if (headers.Host() == nullptr) {
                    return nullptr;
                  }
                 
                  // TODO (@rshriram) Match Origin header in WebSocket
                  // request with VHost, using wildcard match
                  // Lower-case the value of the host header, as hostnames are case insensitive.
                  const std::string host = absl::AsciiStrToLower(headers.getHostValue());
                  const auto& iter = virtual_hosts_.find(host);
                  if (iter != virtual_hosts_.end()) {
                    return iter->second.get();
                  }
                  if (!wildcard_virtual_host_suffixes_.empty()) {
                    const VirtualHostImpl* vhost = findWildcardVirtualHost(
                        host, wildcard_virtual_host_suffixes_,
                        [](const std::string& h, int l) -> std::string { return h.substr(h.size() - l); });
                    if (vhost != nullptr) {
                      return vhost;
                    }
                  }
                  if (!wildcard_virtual_host_prefixes_.empty()) {
                    const VirtualHostImpl* vhost = findWildcardVirtualHost(
                        host, wildcard_virtual_host_prefixes_,
                        [](const std::string& h, int l) -> std::string { return h.substr(0, l); });
                    if (vhost != nullptr) {
                      return vhost;
                    }
                  }
                  return default_virtual_host_.get();
                }
                ```
                
            4.  在一个virtualhost上查找对应route和cluster
                
                *   在通过domain匹配到virtualhost，会在那个virtualhost上匹配查找cluster，如果没匹配上，会直接返回404
                
                *   match可以根据配置分为`prefix`,`regex`,`path`三种route进行匹配
                    
                *   如果存在`weighted_clusters`，会根据`stream_id`, 和clusters的weight进行分发，`stream_id`本身是每个请求独立随机生成，所以`weighted_clusters`的权重分发可以视为随机分发
                    
        2.    
            
            1.  没有route能匹配请求
                
                返回 404`no cluster match for URL`
                
            2.  有配置`directResponseEntry`
                
                直接返回
                
            3.  route上的clustername在clustermanager上找不到对应cluster
                
                返回配置的`clusterNotFoundResponseCode`
                
            4.  当前处于`maintenanceMode (和主动健康检查相关)`
                
            5.  ```
                // See if we are supposed to immediately kill some percentage of this cluster's traffic.
                  if (cluster_->maintenanceMode()) {
                    callbacks_->streamInfo().setResponseFlag(StreamInfo::ResponseFlag::UpstreamOverflow);
                    chargeUpstreamCode(Http::Code::ServiceUnavailable, nullptr, true);
                    callbacks_->sendLocalReply(
                        Http::Code::ServiceUnavailable, "maintenance mode",
                        [modify_headers, this](Http::ResponseHeaderMap& headers) {
                          if (!config_.suppress_envoy_headers_) {
                            headers.addReference(Http::Headers::get().EnvoyOverloaded,
                                                 Http::Headers::get().EnvoyOverloadedValues.True);
                          }
                          // Note: append_cluster_info does not respect suppress_envoy_headers.
                          modify_headers(headers);
                        },
                        absl::nullopt, StreamInfo::ResponseCodeDetails::get().MaintenanceMode);
                    cluster_->stats().upstream_rq_maintenance_mode_.inc();
                    return Http::FilterHeadersStatus::StopIteration;                                                                                                                                                       
                  }
                ```
                
            6.  调用createConnPool获取upstream conn pool
                
            7.    
                
                1.  ```
                    std::unique_ptr<GenericConnPool> generic_conn_pool = createConnPool(*cluster);
                     
                    if (!generic_conn_pool) {
                      sendNoHealthyUpstreamResponse();
                      return Http::FilterHeadersStatus::StopIteration;
                    }
                    ```
                    
                2.  根据 cluster上的`features`配置和`USE_DOWNSTREAM_PROTOCOL`来确定使用http1还是http2协议向上游发送请求
                3.  ```
                    std::vector<Http::Protocol>
                    ClusterInfoImpl::upstreamHttpProtocol(absl::optional<Http::Protocol> downstream_protocol) const {
                      if (downstream_protocol.has_value() &&
                          features_ & Upstream::ClusterInfo::Features::USE_DOWNSTREAM_PROTOCOL) {
                        return {downstream_protocol.value()};
                      } else if (features_ & Upstream::ClusterInfo::Features::USE_ALPN) {
                        ASSERT(!(features_ & Upstream::ClusterInfo::Features::HTTP3));
                        return {Http::Protocol::Http2, Http::Protocol::Http11};
                      } else {
                        if (features_ & Upstream::ClusterInfo::Features::HTTP3) {
                          return {Http::Protocol::Http3};
                        }
                        return {(features_ & Upstream::ClusterInfo::Features::HTTP2) ? Http::Protocol::Http2
                                                                                     : Http::Protocol::Http11};
                      }
                    }
                    ```
                    
                4.  在**ThreadLocalClusterManager**上根据cluster name查询cluster
                5.  ```
                    Http::ConnectionPool::Instance*
                    ClusterManagerImpl::ThreadLocalClusterManagerImpl::ClusterEntry::connPool(
                        ResourcePriority priority, absl::optional<Http::Protocol> downstream_protocol,
                        LoadBalancerContext* context, bool peek) {
                      HostConstSharedPtr host = (peek ? lb_->peekAnotherHost(context) : lb_->chooseHost(context));
                      if (!host) {
                        if (!peek) {
                          ENVOY_LOG(debug, "no healthy host for HTTP connection pool");
                          cluster_info_->stats().upstream_cx_none_healthy_.inc();
                        }
                        return nullptr;
                      }
                    ```
                    
                6.  根据loadbalancer算法挑选节点（此处worker之间的负载均衡根据不同的负载均衡算法有的是独立的，比如round robin，只有同一个Worker上的才是严格的顺序）  
                7.  ```
                    HostConstSharedPtr LoadBalancerBase::chooseHost(LoadBalancerContext* context) {
                      HostConstSharedPtr host;
                      const size_t max_attempts = context ? context->hostSelectionRetryCount() + 1 : 1;
                      for (size_t i = 0; i < max_attempts; ++i) {
                        host = chooseHostOnce(context);
                     
                        // If host selection failed or the host is accepted by the filter, return.
                        // Otherwise, try again.
                        // Note: in the future we might want to allow retrying when chooseHostOnce returns nullptr.
                        if (!host || !context || !context->shouldSelectAnotherHost(*host)) {
                          return host;
                        }
                      }
                     
                      // If we didn't find anything, return the last host.
                      return host;
                    }
                    ```
                    
                8.  根据节点和协议拿到连接池 (连接池由ThreadLocalClusterManager管理，各个Worker不共享)
                9.  没有做直接503,中止解析链
                10.  ```
                     std::unique_ptr<GenericConnPool> generic_conn_pool = createConnPool(*cluster);
                      
                     if (!generic_conn_pool) {
                       sendNoHealthyUpstreamResponse();
                       return Http::FilterHeadersStatus::StopIteration;
                     }
                     ```
                    
            8.  根据配置（timeout, perTryTimeout）确定本次请求的timeout
                
            9.  ```
                timeout_ = FilterUtility::finalTimeout(*route_entry_, headers, !config_.suppress_envoy_headers_,
                                                         grpc_request_, hedging_params_.hedge_on_per_try_timeout_,
                                                         config_.respect_expected_rq_timeout_);
                 
                imeoutData timeout;
                  if (!route.usingNewTimeouts()) {
                    if (grpc_request && route.maxGrpcTimeout()) {
                      const std::chrono::milliseconds max_grpc_timeout = route.maxGrpcTimeout().value();
                      auto header_timeout = Grpc::Common::getGrpcTimeout(request_headers);
                      std::chrono::milliseconds grpc_timeout =
                          header_timeout ? header_timeout.value() : std::chrono::milliseconds(0);
                      if (route.grpcTimeoutOffset()) {
                        // We only apply the offset if it won't result in grpc_timeout hitting 0 or below, as
                        // setting it to 0 means infinity and a negative timeout makes no sense.
                        const auto offset = *route.grpcTimeoutOffset();
                        if (offset < grpc_timeout) {
                          grpc_timeout -= offset;
                        }
                      }
                 
                      // Cap gRPC timeout to the configured maximum considering that 0 means infinity.
                      if (max_grpc_timeout != std::chrono::milliseconds(0) &&
                          (grpc_timeout == std::chrono::milliseconds(0) || grpc_timeout > max_grpc_timeout)) {
                        grpc_timeout = max_grpc_timeout;
                      }
                      timeout.global_timeout_ = grpc_timeout;
                    } else {
                      timeout.global_timeout_ = route.timeout();
                    }
                  }
                  timeout.per_try_timeout_ = route.retryPolicy().perTryTimeout();
                ```
                
            10.  把之前生成的trace写入request header
                
            11.  对request做一些最终的修改，`headers_to_remove``headers_to_add``host_rewrite``rewritePathHeader(路由的配置)`
                
            12.  ```
                 route_entry_->finalizeRequestHeaders(headers, callbacks_->streamInfo(),
                                                      !config_.suppress_envoy_headers_);
                 ```
                
            13.  构造 retry和shadowing的对象
                
            14.  ```
                 retry_state_ = createRetryState(
                       route_entry_->retryPolicy(), headers, *cluster_, request_vcluster_, config_.runtime_,
                       config_.random_, callbacks_->dispatcher(), config_.timeSource(), route_entry_->priority());
                  
                   // Determine which shadow policies to use. It's possible that we don't do any shadowing due to
                   // runtime keys.
                   for (const auto& shadow_policy : route_entry_->shadowPolicies()) {
                     const auto& policy_ref = *shadow_policy;
                     if (FilterUtility::shouldShadow(policy_ref, config_.runtime_, callbacks_->streamId())) {
                       active_shadow_policies_.push_back(std::cref(policy_ref));
                     }
                   }
                 ```
                
    ### 发送请求
    
    发送请求部分也是在`envoy.router`中的逻辑
    
    1.  查看当前conn pool是否有空闲client
    2.  ```
        if (!ready_clients_.empty()) {
           ActiveClient& client = *ready_clients_.front();
           ENVOY_CONN_LOG(debug, "using existing connection", client);
           attachStreamToClient(client, context);
              // Even if there's a ready client, we may want to preconnect to handle the next incoming stream.
           tryCreateNewConnections();
        ```
        
        *   如果存在空闲连接
            1.  根据downstream request和tracing等配置构造发往upstream的请求buffer
            2.  把buffer一次性移入`write_buffer_`, 立即触发Write Event
            3.  `ConnectionImpl::onWriteReady`随后会被触发
            4.  把`write_ buffer_`的内容写入socket发送出去
        *   如果不存在空闲连接
        *   ```
            if (host_->cluster().resourceManager(priority_).pendingRequests().canCreate()) {
                ConnectionPool::Cancellable* pending = newPendingStream(context);
                ENVOY_LOG(debug, "trying to create new connection");
                ENVOY_LOG(trace, fmt::format("{}", *this));
             
                auto old_capacity = connecting_stream_capacity_;
                // This must come after newPendingStream() because this function uses the
                // length of pending_streams_ to determine if a new connection is needed.
                const ConnectionResult result = tryCreateNewConnections();
                // If there is not enough connecting capacity, the only reason to not
                // increase capacity is if the connection limits are exceeded.
                ENVOY_BUG(pending_streams_.size() <= connecting_stream_capacity_ ||
                              connecting_stream_capacity_ > old_capacity ||
                              result == ConnectionResult::NoConnectionRateLimited,
                          fmt::format("Failed to create expected connection: {}", *this));
                return pending;
              } else {
                ENVOY_LOG(debug, "max pending streams overflow");
                onPoolFailure(nullptr, absl::string_view(), ConnectionPool::PoolFailureReason::Overflow,
                              context);
                host_->cluster().stats().upstream_rq_pending_overflow_.inc();
                return nullptr;
              }
            ```
            
            1.  根据`max_pending_requests`和`max_connections`判断是否可以创建新的连接（此处的指标为worker间共享），但是每个线程会向上游最少建立一条连接，也就是极端策略可能需要和工作线程数相关
            2.  根据配置设置新连接的socket options, 使用`dispatcher.createClientConnection`创建连接上游的连接，并绑定到eventloop
            3.  新建`PendingRequest`并加到`pending_requests_`头部
            4.  当连接成功建立的时候，会触发`ConnectionImpl::onFileEvent`
            5.  在`onConnected`的回调中
                1.  停止`connect_timer_`
                2.  复用存在空闲连接时的逻辑，发送请求
    3.  在`onRequestComplete`里调用`maybeDoShadowing`进行流量复制
    4.  ```
        ASSERT(!request->headers().getHostValue().empty());
          // Switch authority to add a shadow postfix. This allows upstream logging to make more sense.
          auto parts = StringUtil::splitToken(request->headers().getHostValue(), ":");
          ASSERT(!parts.empty() && parts.size() <= 2);
          request->headers().setHost(parts.size() == 2
                                         ? absl::StrJoin(parts, "-shadow:")
                                         : absl::StrCat(request->headers().getHostValue(), "-shadow"));
          // This is basically fire and forget. We don't handle cancelling.
          thread_local_cluster->httpAsyncClient().send(std::move(request), *this, options);
        ```
        
        1.  shadowing流量并不会返回错误
        2.  shadowing 流量为asynclient发送，不会阻塞downstream，timeout也为`global_timeout_`
        3.  shadowing 会修改request header里的host 和 authority 添加`-shadow`后缀
    5.  根据`global_timeout_`启动响应超时的定时器
    
    ###  接收响应
    
    1.  eventloop 触发`ClientConnectionImpl.ConnectionImpl`上的`onFileEvent`的read ready事件
    2.  经过http\_parser execute后触发`onHeadersComplete`后执行到`UpstreamRequest::decodeHeaders`
    3.  `upstream_request_->upstream_host_->outlierDelector().putHttpResponseCode`写入status code，更新外部检测的状态
    4.  ```
        external_origin_sr_monitor_.incTotalReqCounter();
          if (Http::CodeUtility::is5xx(response_code)) {
            std::shared_ptr<DetectorImpl> detector = detector_.lock();
            if (!detector) {
              // It's possible for the cluster/detector to go away while we still have a host in use.
              return;
            }
            if (Http::CodeUtility::isGatewayError(response_code)) {
              if (++consecutive_gateway_failure_ ==
                  detector->runtime().snapshot().getInteger(
                      ConsecutiveGatewayFailureRuntime, detector->config().consecutiveGatewayFailure())) {
                detector->onConsecutiveGatewayFailure(host_.lock());
              }
            } else {
              consecutive_gateway_failure_ = 0;
            }
         
            if (++consecutive_5xx_ == detector->runtime().snapshot().getInteger(
                                          Consecutive5xxRuntime, detector->config().consecutive5xx())) {
              detector->onConsecutive5xx(host_.lock());
            }
          } else {
            external_origin_sr_monitor_.incSuccessReqCounter();
            consecutive_5xx_ = 0;
            consecutive_gateway_failure_ = 0;
          }
        ```
        
    5.  ```
        if (grpc_status.has_value()) {
          upstream_request.upstreamHost()->outlierDetector().putHttpResponseCode(grpc_to_http_status);
        } else {
          upstream_request.upstreamHost()->outlierDetector().putHttpResponseCode(response_code);
        }
        ```
        
    6.  根据返回结果、配置和`retries_remaining_`判断是否应该retry，参考文档  [默认的retry策略](/pages/viewpage.action?pageId=98888759) 
        
        1.  根据`internal_redirect_action`的配置和response来确定是否需要redirect到新的host
        2.  ```
            InternalRedirectPolicyImpl RouteEntryImplBase::buildInternalRedirectPolicy(
                const envoy::config::route::v3::RouteAction& route_config,
                ProtobufMessage::ValidationVisitor& validator, absl::string_view current_route_name) const {
              if (route_config.has_internal_redirect_policy()) {
                return InternalRedirectPolicyImpl(route_config.internal_redirect_policy(), validator,
                                                  current_route_name);
              }
              envoy::config::route::v3::InternalRedirectPolicy policy_config;
              switch (route_config.internal_redirect_action()) {
              case envoy::config::route::v3::RouteAction::HANDLE_INTERNAL_REDIRECT:
                break;
              case envoy::config::route::v3::RouteAction::PASS_THROUGH_INTERNAL_REDIRECT:
                FALLTHRU;
              default:
                return InternalRedirectPolicyImpl();
              }
              if (route_config.has_max_internal_redirects()) {
                *policy_config.mutable_max_internal_redirects() = route_config.max_internal_redirects();
              }
              return InternalRedirectPolicyImpl(policy_config, validator, current_route_name);
            }
             
              if (num_internal_redirect.value() >= policy.maxInternalRedirects()) {
                config_.stats_.passthrough_internal_redirect_too_many_redirects_.inc();
                return false;
              }
            ```
          
        ### 返回响应  
        1.  停止`request_timer`, 重置`idle_timer`
        2.  和向upstream发送请求一样的逻辑，发送响应给downstream

  

  

###阅读源码总结：

1. envoy当中各种继承，模板，组合使用的非常多，子类初始化时需要关注父类的构造函数做了什么
2. 可以根据请求日志的信息，通过日志的顺序再到代码走一遍大体过程
3. 善用各种调试工具，例如抓包，gdb，放开指标等，个人的经验 百分之90的问题日志+抓包+部分源码的阅读可以解决

  

  

####附录：
[https://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.2](https://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.2)   关于重复header的rfc规范
[https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http\_conn\_man/header\_casing](https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/header_casing)    关于header大小写处理
[https://www.envoyproxy.io/docs/envoy/latest/version\_history/v1.15.1](https://www.envoyproxy.io/docs/envoy/latest/version_history/v1.15.1)     关于修改header append行为